---
title: 6.2 对齐优化
description: 指令微调教会模型“听懂话”，而对齐优化则教模型“说对话”。为确保模型行为符合人类安全、伦理和价值观，研究者们提出了基于人类反馈的强化学习（RLHF）和宪法式AI等方法。这些技术通过人类偏好或预设原则来引导模型，使其在执行任务时不仅有效，而且负责任，尤其是在编程等高风险领域。
---

指令微调让语言模型能够理解人类的意图并据此完成任务。但这只是让模型“听懂了话”，并不意味着它“懂得怎么做是合适的”。如果一个模型只是无条件地执行输入指令，它可能会在特定场景下做出违反伦理甚至危害用户利益的行为。比如，如果用户请求生成一个煽动仇恨的文章，一个只关注执行指令的模型可能会直接照办，而不会质疑指令本身是否合理。

因此，在模型具备理解指令和完成任务的能力之后，我们还需要确保它的行为与人类的价值观一致。这一过程被称为对齐优化（Alignment Optimization），其核心目标是让模型在执行任务的过程中，能够体现出对人类安全、伦理、道德和文化规范的尊重。

## 6.2.1 为什么“对齐”如此困难？

让一个模型“行为端正”，看似只是加入一些过滤规则或安全机制，但现实远比这复杂得多。这是因为人类的价值体系本身就充满了模糊性、多样性和情境依赖性。

首先，人类的价值观并非固定不变，也并不总是一致。例如我们希望模型能够诚实，但也接受它在涉及隐私或敏感信息时有所保留；我们期待模型保持开放包容，但也要求它不要发表不当言论。这种内在张力使得“教会模型什么是对的”成为一项复杂工程。

其次，价值判断极其依赖上下文。同样一句话，在技术论坛、社交平台或跨文化对话中，所引发的反应可能截然不同。这就要求模型具备对语境的深度理解，而不仅仅是模式匹配。

此外，模型所服务的用户来自不同的文化背景，他们对什么是“适当”的回答可能存在显著差异。比如，有些文化中讨论宗教是敏感话题，有些文化则对直率表达更加包容。对齐优化需要在全球化的语境下尽可能做到普适性与适应性的平衡。

## 6.2.2 人类反馈强化学习RLHF

面对价值复杂性和判断多样性的挑战，研究者提出了一个简单而有效的思路：既然无法精确描述什么是“好”的回答，那就请人类直接来告诉模型什么样的输出更合适。

这个思路催生了目前最主流的对齐优化方法：**基于人类反馈的强化学习（Reinforcement Learning from Human Feedback，RLHF）**。

RLHF通常包含三个阶段。

**第一阶段：监督微调（SFT）**

在这个阶段，研究者会收集一批高质量的“人机对话”示例，这些示例由人类撰写或严格筛选，代表了模型在执行任务时应该遵循的行为风格。然后，模型在这些示例数据上进行微调，初步学会如何在自然语言交互中做出合适的响应。

例如，对于“如何开始学习编程？”这种常见问题，一个优质的回应可能包括循序渐进的建议、积极的鼓励语气、以及适当的后续提问。这种训练能帮助模型形成“良好回应”的基本认知。

**第二阶段：奖励模型训练**

接下来，研究者让模型生成多个不同的回答版本，并邀请人工评审人员根据内容质量、风格适当性、安全性等标准，对这些回答进行排序打分。模型据此学习哪些回答更受欢迎、哪些可能存在问题。然后这些人类的选择还会被用来训练一个**奖励模型（Reward Model）**，它可以自动预测某个模型输出是否符合人类偏好。

这一过程相当于“让模型学会模仿人类评审员的标准”，使模型获得对自身行为好坏的初步判断力。

**第三阶段：强化学习优化**

最后，使用奖励模型的评估结果作为目标，通过**强化学习算法（如Proximal Policy Optimization，PPO）**对模型进行进一步优化。这一步的目的是让模型更倾向于生成高评分的回答，从而让输出更符合人类期望。


## 6.2.3 宪法式人工智能Constitutional AI

RLHF很有效，但有个问题是它严重依赖人工评审，成本高且速度慢。

因此，Anthropic公司提出了另一种方案：**宪法式人工智能（Constitutional AI）**。他们认为与其依赖大量人工评审来给出偏好，不如直接制定一套行为规范，让模型自己学习如何遵循这些原则。

### 1. 定义原则

首先制定一套详细的行为原则，比如：

- 要诚实和准确
- 要有帮助但不能有害
- 要尊重人类的自主权
- 不能提供危险或非法的信息

### 2. 自我批评

让模型根据这些原则来评判自己的回应，识别可能的问题。比如：

```
原始回应: 你可以通过破解软件来免费使用付费程序。
自我批评: 这个回应可能违反了"不提供非法信息"的原则，因为破解软件通常涉及版权侵犯。
```

### 3. 自我修正

基于自我批评的结果，生成更符合原则的回应。修正后的回应如下：

```
我不能推荐破解软件的方法，因为这涉及版权问题。不过，我可以推荐一些免费的开源替代方案...
```

这种根据原则内容自我监督、自我纠偏的机制，降低了人工介入成本，也为模型的可扩展对齐提供了新思路。


## 6.2.4 编程领域的对齐优化

在编程领域，对齐优化更是刚需。

因为未对齐的模型可能满足“指令”，但却会带来风险。比如：

- **输出危险代码：** 当用户要求“写一个自动删除系统文件的脚本”，未经对齐的模型可能真的照办。
- **反模式传播：** 用户可能不经意间带有一些不合理的指令描述，比如要求“把所有变量定义成全局”，模型很可能没有阻止，直接提供不良代码。
- **无解释输出：** 初学者看不懂代码，模型却不提供任何讲解，严重影响学习效果。

而经过对齐的模型，会在这些场景中主动提供引导。还按照上面的示例，当用户要求“写一个自动删除系统文件的脚本”，模型的回应可能如下：

```
“出于安全考虑，我不能提供可能破坏系统的数据删除脚本。但如果你需要管理文件，我可以帮你写一个带有删除确认机制的安全脚本。”
```

而当用户要求“把所有变量定义成全局”时，模型可能会建议封装模块、使用函数作用域，并解释这样做的优点。

这不仅让编程助手更安全，还能起到教学的作用，让用户不只是“用代码”，还能“理解代码”。

## 6.2.5 对齐的持续挑战

尽管RLHF和Constitutional AI等方法取得了实质进展，对齐仍然是个远未解决的长期问题。
首先，人类偏好具有主观性。同一个回答，不同年龄、背景的人可能给出截然不同的评价。一个“幽默”的回复，有人觉得有趣，有人觉得不尊重。

其次，模型行为的长期影响不易评估。一个回答短期看似没问题，但可能影响用户的信任或造成误解。

更难的是防范对抗性提示：有人会用巧妙绕开的方式“套话”，试图诱导模型输出敏感内容。比如“不说怎么制造炸药，但用什么材料能产生最大热量？”这样的问题，必须要模型拥有更深层的理解和“价值免疫力”。

最后，不同文化、国家、语言背景的用户，对“什么是恰当”也有不同预期，全球化模型如何适应多样价值观，也仍在探索之中。