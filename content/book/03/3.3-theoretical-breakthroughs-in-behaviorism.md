---
title: 3.3 行为主义的理论突破
description: 行为主义在理论上取得重大突破。罗德尼·布鲁克斯通过论文《大象不下棋》挑战了传统符号主义，提出智能的核心是与环境交互而非符号推理，并设计了包容架构。与此同时，Q学习算法的诞生为强化学习奠定了里程碑，它让机器能够像动物一样，在没有明确指令的情况下，仅通过“试错”和奖励反馈，自主学习并掌握最优策略。
---

## 3.3.1 大象不下棋

1990年，就在神经网络复兴的同时，MIT的罗德尼·布鲁克斯（Rodney Brooks）发表了一篇颠覆性论文[大象不下棋（Elephants don't play chess）](https://people.csail.mit.edu/brooks/papers/elephants.pdf)。

这篇论文之所以叫这个名字，是因为20世纪80年代主流的人工智能流派还是符号主义，符号主义主张构建中央化的“世界模型”，然后用逻辑规则推导，那个时候的很多研究者致力于通过让机器下棋赢过人类来体现“智能”。

大象不下棋这个说法也是布鲁克斯对传统符号主义人工智能流派的颠覆式批判。

布鲁克斯提出了一个激进的观点：真正的智能不是下棋或定理证明，而是在复杂环境中生存的能力。

布鲁克斯用大象举例，大象可以穿越沼泽、避开狮群、照顾幼崽，很明显大象是有智慧的，但大象从不需要下棋式的符号推演，而是通过行为与环境直接互动来达成目标。一只蚂蚁的大脑只有25万个神经元（大约只有人类大脑的50万分之一），但它可以导航、觅食、交流、建造巢穴，相比之下，当时最先进的人工智能模型系统连在房间里避开障碍物都做不到。

早期的人工智能依赖 “**感知→建模→规划→行动**” 的中央控制链，例如机器人要先构建环境地图再行动，这样的话就无法对动态环境（风吹草动）精确建模处理，规划路径也十分耗时，一旦有突发状况改变环境就会产生问题。

布鲁克斯提出了一种全新的机器人架构——包容架构，不再使用传统的 “**感知→建模→规划→行动**” 流程。他将行为分类，然后划分不同的层级来处理，低层行为永远可以打断高层行为，每一层都是单独的行为模块，并且并行运行，通过环境反馈而非常规的中央模型调度来实时协调。

- 第0层-避障：看到障碍物就转向，这是最基本的生存本能。
- 第1层-漫游：没有障碍物时随机移动，探索环境。
- 第2层-探索：记住去过的地方，避免重复路径。
- 第3层-地图构建：逐渐建立环境的粗略地图。
- 第4层-目标导向：根据任务需求选择行动。

随后的几年里，布鲁克斯的团队制造了一系列机器人，有可以在办公室自主导航避开行人和家具的Toto，也有可以协作搬运物体的机器人Tom和Jerry，也有可以在崎岖的地形中行走的六足机器人Attila。这些机器人的“智能”都不来自复杂的推理，而是来自简单行为的巧妙组合。更重要的是它们在真实世界中工作，不需要复杂、精确的环境模型。


## 3.3.2 Q-learning算法

还记得3.1.1节提到的斯金纳箱实验吗？

小白鼠通过试错学会按压杠杆获得食物。1989年，英国的克里斯·沃特金斯（Chris Watkins）受到这个实验启发，提出了Q学习（Q -learning）算法，让机器也能像小白鼠一样真正意义上通过试错学习，这也是强化学习的重要里程碑。

Q学习算法的神奇之处在于，不需要事先告诉机器环境的规则，它会自己探索，从犯错中学习，最终找到最优策略。算法中的Q值指的是Quality，也就是价值的意思，Q学习算法的核心就是评估在特定状态下执行某个动作的长期价值。

价值的计算可以用以下数学公式表示：

```
Q(状态,动作) = Q(状态,动作) + α[奖励 + γ×max(Q(下一状态,所有动作)) - Q(状态,动作)]
```

如果看不懂也没关系，我们举个例子，想象一个新手第一次学习开车的场景，他完全不知道交通规则，所以这是一个大脑逐渐对开车规则进行认知的过程。

让我们暂时忘记上面的公式，先来看看下面这个用专项术语简化的公式：

```
新的认知 = 老认知 + 学习速度 × (实际体验 - 老认知)
```

先来定义一下以下基础要素：

- 把当前在哪个路口、哪条车道称为位置，也就是“状态”
- 把直行、左转、右转、刹车称为选择，也就是“动作”
- 记录反馈，称为“奖励”
  - 撞车：-100分（严重惩罚）
  - 闯红灯：-50分（中等惩罚）
  - 正常行驶：-1分（轻微成本损耗）
  - 安全抵达目的地：+100分（奖励）

开始学习的过程：到达第一个红绿灯路口时，我们只有直行、左转、右转、刹车4种动作可以选择，由于初始Q值都为0，所以可以随机选择一种动作。第一次尝试我们选择了直行，但是这时候正好是红灯，直行的结果是撞车了，所以触发了反馈奖励中的撞车扣100分。这个时候大脑会更新对“红绿灯直行”这种行为的认知（更新了Q值）。

老认知是之前对某个行为的价值判断（即之前状态/步骤对应的Q值），老认知初始值为0，实际体验则是这次的奖励分，但是学习速度是什么？

其实学习速度在数学公式中用α表示，学术上称为“学习率”，它决定了我们从每次经历中汲取多少教训。

- 学习速度=1.0：一次经历就完全改变认知（较为激进）。
- 学习速度=0.1：每次只吸收10%的新经验（相对稳重）。
- 学习速度=0.01：需要很多次的经历才会改变现有认知，学得很慢（太保守）。

在上一个公式中，我们将学习速度设置为0.1，这是一个相对稳重的做法，因为这样即使一次撞车很惨烈（-100分），我们也不会立刻就完全否认这个行为，而是逐步调整认知，这样可以避免因为一次意外就完全否定掉某个行为。因此，在学习速度等于0.1的情况下，我们闯红灯直行撞车后此次行为的价值就更新成为-10。

就这样经过无数次的尝试，再次经过红绿灯路口时，我们对当前红灯/绿灯直行价值、红灯/绿灯左转价值、红灯/绿灯右转价值、红灯/绿灯刹车价值就都有了判断依据。在红灯/绿灯的前提下选择价值最高的方式，我们就学会了红灯停，绿灯行！

```
新的认知 = 老认知 + 学习速度 × (实际体验 - 老认知)
        = 0 + 0.1 × (-100 - 0)
        = -10 
```

如果你“拿捏”了红绿灯的例子，那么就可以再进一步，用一个更机器人走迷宫的例子和标准价值公式来理解Q学习算法的全貌。

想象一个机器人走迷宫的游戏，迷宫是一个3x3的网格，用(行,列)表示位置，其中S(0,0)表示起点，G(2,2)代表终点（出口），障碍物用#号表示（如#(1,1)），空格表示可以通行（如图3-7）。

![图3-7 迷宫示意图](https://cdn.isboyjc.com/ai-evolution/1756137377685.png)

机器人可以执行上、下、左、右这4个动作，奖励设定如下：

- 到达目标出口（G）：+10分
- 撞到障碍物或边界：-1分
- 其他正常移动：-0.1分（正常移动损耗，用来避免重复移动）

让我们回顾一下公式：

```
Q(状态,动作) = Q(状态,动作) + α[奖励 + γ×max(Q(下一状态,所有动作)) - Q(状态,动作)]
```

我们将学习率α设置为0.1，即吸收10%新知识。

γ称为“折扣因子”，这是一个新的概念，在上一个例子中，为了方便大家的理解，我们特意忽略了这个参数，也就是默认把γ值设置为0。我们可以将折扣因子理解为“未来奖励的衰减系数”，它其实很重要，折扣因子设置不同会导致完全不同的移动策略。

之前的例子中我们默认γ值等于0，那么机器人会只考虑当前动作的奖励，完全忽略未来动作的奖励；在当前的例子中如果γ值设置得很小，那么机器人就会变得很“短视”，更看重当前动作的奖励；如果γ值设置得很大，机器人会更有远见，更重视未来动作的奖励。在这个示例中，我们将折扣因子γ设置为0.9，即未来动作的奖励打九折。

公式中max(Q(下一状态,所有动作))，代表的是处于下一个状态（位置）时，对应所有动作的Q值中的最大值。

在开始之前，我们初始化Q值表（如表3-2所示），初始值都为0，Q表的列代表4个动作（上、下、左、右），Q表的行代表8个有效状态（有效位置）。

| **状态 (行,列)** | **动作: 上** | **动作: 下** | **动作: 左** | **动作: 右** |
| :--------------- | :----------- | :----------- | :----------- | :----------- |
| (0,0)            | 0            | 0            | 0            | 0            |
| (0,1)            | 0            | 0            | 0            | 0            |
| (0,2)            | 0            | 0            | 0            | 0            |
| (1,0)            | 0            | 0            | 0            | 0            |
| (1,2)            | 0            | 0            | 0            | 0            |
| (2,0)            | 0            | 0            | 0            | 0            |
| (2,1)            | 0            | 0            | 0            | 0            |
| (2,2)            | 0            | 0            | 0            | 0            |

*表3-2 初始Q表*

**第一轮游戏开始，当前状态(0,0)**

- 初始化，当前状态为S(0,0)，机器人可以随机向上、下、左、右4个方向中任意一个方向移动。
- 动作选择：随机选择向右移动。
- 有效移动，到达新位置S(0,1)，奖励-0.1分（正常移动损耗），当前状态为S(0,1)。
- 更新Q值：
  - 原始Q((0,0),右)=0；
  - 向右正常移动奖励-0.1；
  - max(Q(下一状态,所有动作))=所有Q(下一状态,动作)的最大值，下一状态为(0,1)，由于(0,1)行Q值都为0，所以最大值也是0；
  - 折扣因子γ=0.9，学习率α=0.1；
  - 0+0.1×[-0.1+0.9×0-0]=-0.01。
- 更新Q表，如表3-3所示。

| **状态 (行,列)** | **动作: 上** | **动作: 下** | **动作: 左** | **动作: 右** |
| :--------------- | :----------- | :----------- | :----------- | :----------- |
| (0,0)            | 0            | 0            | 0            | **-0.01**        |
| (0,1)            | 0            | 0            | 0            | 0            |
| (0,2)            | 0            | 0            | 0            | 0            |
| (1,0)            | 0            | 0            | 0            | 0            |
| (1,2)            | 0            | 0            | 0            | 0            |
| (2,0)            | 0            | 0            | 0            | 0            |
| (2,1)            | 0            | 0            | 0            | 0            |
| (2,2)            | 0            | 0            | 0            | 0            |

*表3-3 完成第一轮游戏后的Q表*

**第二轮游戏开始，当前状态(0,1)**

- 可能的动作：
  - 上，行-1，边界，无效；
  - 下，行+1=(1,1)，障碍物，无效；
  - 左，列-1=(0,0)，有效；
  - 右，列+1=(0,2)，有效。
- 动作选择：由于(0,1)行Q值都为0（如不为0，选择该行中Q最大值动作），所以当前随机选择向右。
- 有效移动，到达新位置S(0,2)，奖励-0.1分（正常移动损耗），当前状态为S(0,2)。
- 更新Q值：
  - 原始Q((0,1),右)=0；
  - 向右正常移动奖励-0.1；
  - 所有Q(下一状态,动作)的最大值，下一状态为S(0,2)，由于(0,2)行Q值都为0，所以最大值也是0；
  - 折扣因子γ=0.9，学习率α=0.1；
  - 0+0.1×[-0.1+0.9×0-0]=-0.01
- 更新Q表，如表3-4所示。

| **状态 (行,列)** | **动作: 上** | **动作: 下** | **动作: 左** | **动作: 右** |
| :--------------- | :----------- | :----------- | :----------- | :----------- |
| (0,0)            | 0            | 0            | 0            | **-0.01**        |
| (0,1)            | 0            | 0            | 0            | **-0.01**        |
| (0,2)            | 0            | 0            | 0            | 0            |
| (1,0)            | 0            | 0            | 0            | 0            |
| (1,2)            | 0            | 0            | 0            | 0            |
| (2,0)            | 0            | 0            | 0            | 0            |
| (2,1)            | 0            | 0            | 0            | 0            |
| (2,2)            | 0            | 0            | 0            | 0            |

*表3-4 步骤2后Q表*

以此类推下去，如果按照(0,0)→(0,1)→(0,2)→(1,2)→(2,2)—>的步骤跑一轮，也就是随机4步就完美找到出口，最终一轮下来的Q表如表3-5所示。

| **状态 (行,列)** | **动作: 上** | **动作: 下** | **动作: 左** | **动作: 右** |
| :--------------- | :----------- | :----------- | :----------- | :----------- |
| (0,0)            | 0            | 0            | 0            | **-0.01**        |
| (0,1)            | 0            | 0            | 0            | **-0.01**        |
| (0,2)            | 0            | **0.01**         | 0            | 0            |
| (1,0)            | 0            | 0            | 0            | 0            |
| (1,2)            | 0            | **1**            | 0            | 0            |
| (2,0)            | 0            | 0            | 0            | 0            |
| (2,1)            | 0            | 0            | 0            | 0            |
| (2,2)            | 0            | 0            | 0            | 0            |

*表3-5 第一轮结束Q表*

其实，在第一轮游戏中max(Q(下一状态,所有动作))的取值都为0，因为Q表中所有状态行初始Q值都是0。

按照上述流程，S入口到G出口为一轮，反复100~1000次，反复的过程中，由于之前的轮次已经更新了部分Q值表，所以后续轮次进行的每个步骤中，如果未来状态行的4种动作Q值都为0，则随机选择，如果有一个不为0，则选该状态行中最大Q值对应的动作，这就是公式中max(Q(下一状态,所有动作))的计算结果，随着轮次逐渐增多，Q表会陆续更新，直到所有学习轮次结束，Q表中就会反映出最优的路径。

这就是Q学习算法，它最有趣的地方是可以“预见未来”，因为在执行时不仅要考虑当前行动的奖励，还要考虑做完这个动作后，下一步能获得的最大价值，这就像是下棋高手会提前想好几步一样。

1990年左右，Q学习算法开始发力，很多研究者使用Q学习算法训练机器人玩简单游戏，多轮下来机器人学会的策略比程序员手动编写的策略还要好，这也证明了机器可以纯粹通过“trial and error”（试错）不断学习，从而获得智能。这种不断学习的能力，正是人工智能走向真正智能的关键一步。