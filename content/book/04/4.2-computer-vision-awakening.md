---
title: 4.2 计算机视觉的觉醒
description: 计算机视觉领域因两大关键进展而觉醒：一是李飞飞团队创建的大规模图像数据集 ImageNet，为深度学习提供了充足的数据；二是 2012 年 AlexNet 模型在 ImageNet 挑战赛上取得的压倒性胜利。AlexNet 作为一个深度卷积神经网络（CNN），其成功不仅验证了 CNN 架构的有效性，也开创了使用 GPU 进行并行计算的先河，标志着深度学习时代的全面开启。
---

辛顿的DBN虽然重新点燃了深度学习的希望，但要让深度学习真正改变世界，还需要三个关键要素的汇聚：海量数据、强大算力，以及合适的网络架构。

2006 年到 2012 年间，这三个要素正在悄然成熟。互联网的普及让数据收集成为可能，GPU 的发展提供了前所未有的并行计算能力，而卷积神经网络这一专门为图像设计的架构也在等待着自己的历史时刻。

缺的只是一个契机，一个能将这些要素完美结合的舞台。这个舞台，叫做 ImageNet。

## 4.2.1 李飞飞的“疯狂”想法

2006 年，斯坦福大学华裔女教授李飞飞（如图 4-3）准备做一件在同事们看来“吃力不讨好”的事情——建立一个包含 2000 万张图片的巨大数据库。

![图 4-3 李飞飞（图片来源于ITU Pictures-flickr.com/photos/itupictures/35011409612）](https://cdn.isboyjc.com/ai-evolution/1756137752982.png)

这个数据库涵盖了 2 万多个类别（基于 WordNet 层次结构组织），从“气球”到“斑马”，从“茶壶”到“潜艇”，应有尽有。她给这个项目起名叫 ImageNet。

当时整个人工智能领包括视觉域都聚焦于算法优化，而李飞飞觉得应该从改进数据入手，构建大规模的图像数据集而非仅仅是改进算法。

> WordNet 是由普林斯顿大学认知科学实验室开发的英语词汇语义数据库，其核心特点是将词汇按语义而非字母顺序组织，形成结构化的语义网络。ImageNet 基于 WordNet 的单词数据库构建。

“收集这么多图片有什么用？”

“这不是在浪费时间和资源吗？”

“计算机视觉领域的数据集通常只有几千张图片，PASCAL VOC 也就 2 万张就够了，谁需要这么多？”

> PASCAL VOC（PASCAL Visual Object Classes）是计算机视觉领域的一个经典数据集和基准挑战赛，主要用于目标检测、图像分类和语义分割等任务的研究与评估。

当时很多人觉得她疯了，但李飞飞依然坚定不移，正如她在自传 **《我看见的世界》** 中所说：“真正的科学创新往往诞生于不被看好的领域。”

## 4.2.2 孩子如何学会看世界

李飞飞的想法来自对孩子学习过程的观察：人类小孩在学会说话前，已经看过了成千上万个物体。一个 2 岁的孩子，每天睁开眼睛的 14 个小时里，大脑都在疯狂地处理视觉信息：

- 每秒钟看到大约 30 - 60 个不同的物体
- 每天接收约 150 万 - 300 万个视觉样本
- 到 2 岁时，已经“见过”超过 10 亿个视觉样本

如果我们想让机器真正“看懂”世界，也需要给它看足够多的例子。几千张图片？这连人类婴儿一天看到的都不如！

## 4.2.3 ImageNet工程

这个想法听起来简单，实现起来可一点都不简单。

整个工程分为三个部分：图片收集、人工标注、质量控制。

李飞飞的团队写了大量的爬虫程序，从 Google、Flickr、Yahoo 等网站爬取图片。

更大的挑战是标注，每张照片都需要人工确认标签是否正确。李飞飞想到了一个办法 —— **众包标注平台（Amazon Mechanical Turk）**。

为了确保标注质量，每张图片都要经过 3-5 个不同标注员的确认，只有多数人同意的标注才被采用。

在李飞飞团队最初的目标里，ImageNet 要为每个类别搜集 1000 张不同的照片（1000 张不同的小提琴照片、1000 张不同的德国牧羊犬照片、1000 张不同的抱枕照片），直到涵盖全部 2 万多个类别，也就是一共需要大约 2000 万张图片。

历经了 3 年时间，最终在 2009 年 6 月 ImageNet 初始版本完成，共收集了 1400 万张图片，涵盖了 2.2 万个不同类别，光是“狗”这个类别就有 120 个子类，这些图片选自将近 10 亿张候选图片。来自 167 个国家的 4.8 万多名全球贡献者进行了标注。


## 4.2.4 ImageNet挑战赛

2010 年李飞飞团队启动了 **ImageNet 视觉识别挑战赛（ImageNet Large Scale Visual Recognition Challenge，ILSVRC）**，这也是计算机视觉领域最具影响力的竞赛之一。

基于 ImageNet 的 1400 万张标注图像数据集，ILSVRC 挑战赛使用 ImageNet 子集，1000 个类别，大约 120 万张标注图象进行训练。训练的任务包含图像分类、目标检测、目标定位等等，最终评估指标为 Top-1/Top-5 错误率和平均精度。

- Top-1 错误率指的是模型预测的最高概率类别与真实标签不一致的比例。
- Top-5 错误率指的是模型预测的前 5 个最高概率类别中均不包含真实标签的比例。
- 平均精度指的是衡量模型在所有类别上的平均检测精度。

2010 年即第一年挑战赛仅有 10 支队伍参赛，主流方法还是依赖手工特征和浅层模型这种传统的方法，冠军的 Top-5 错误率是 28.2%。

2011 年即第二年的参赛队伍依旧以传统技术路线为主加以改进，冠军 Top-5 错误率达到了 25.8%。
比赛发起的前两年都比较平淡，大家开始觉得这可能就是计算机视觉的天花板了，毕竟让机器从 1000 个类别中正确识别物体，本身就是一个非常困难的任务。

直到 2012 年，多伦多大学研究生 **亚历克斯·克里泽夫斯基（Alex Krizhevsky）** 和他的导师 **辛顿** 提交了一个叫 [AlexNet](https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) 的神经网络模型。

比赛结果公布后，AlexNet 以 Top-5 错误率 15.3% 夺得冠军，领先了第二名足足 10.8 个百分点。

图像识别领域一片寂静，因为以这个错误率来看，已经不是渐进式的改善了，而是断崖式跨越。就好像百米赛跑，大家都在 11 秒左右，突然有人“不讲武德”跑出了 9.5 秒。

AlexNet 是怎么做到的？

其实它的本质是一个 8 层的 **卷积神经网络（Convolutional Neural Network，CNN）**。

在介绍 CNN 之前，我们先了解下计算机视觉的发展历程。

## 4.2.5 计算机视觉的发展

计算机视觉的发展历程和人工智能的阶段发展基本是一致的，人工智能很多理念被提出、算法被设计出来后，就会逐渐应用到各个子领域，而计算机视觉就属于人工智能领域的一个重要分支。

由于前文介绍了人工智能发展中的基础知识和背景，所以我们这里简单概括一下计算机视觉发展历程即可。计算机视觉的发展可以划分为四个阶段。

1980 年之前，我们称之为**几何匹配**阶段。

早期的计算机视觉研究者认为，识别物体就是找到它们的几何形状和边界。

思路也很简单，使用边缘检测找到物体的轮廓，然后用几何图形去拟合轮廓，最后通过模板匹配的方式去识别物体。

这种方式就像是“描边画”，比如我们用铅笔描出杯子的轮廓，映射到计算机，它也是先试图画出物体的轮廓，然后把轮廓和已知的形状模板去对比。

很明显，当下的我们一眼就可以看出这种方式的缺陷，它只适用于比较简单、规整的几何物体，对复杂的自然场景无能为力。并且轮廓一样的物体就会识别错误，比如杯子和花瓶。

1980-2000 年，我们称之为 **特征工程** 阶段。

研究者们意识到仅仅是靠几何形状远远不够，还需要补充丰富的图像特征。

思路也很简单，人工去设计各种特征，比如图片纹理、图片颜色直方图、图片形状描述符等等。然后为不同类型的视觉任务去设计专门的特征，最后使用统计的方法为图片进行分类。

其实就像侦探破案一样，侦探会观察嫌疑人的身高、体型、穿着、姿态等等特征，然后根据这些线索推断身份。

其实到这里，计算机视觉的应用已经很广泛了，比如分析纹理、医学图像、遥感图像等等领域都在使用特征去识别图像。但是特征设计还是那些缺陷，完全依赖专家的经验，不同的任务需要重新设计，泛化能力非常差。

2000-2012 年，我们称之为 **机器学习** 阶段。

这个阶段的特点是手动特征+机器学习算法的组合。

核心思路虽然还是要依赖人工设计特征，但是会使用支持向量机、随机森林等算法自动学习分类规则。然后通过大量数据的训练提高识别的准确率。

就像培训一个古董鉴定师，要先教他观察古董的材质、工艺、纹样等特征，基础特征了解之后，再给他看大量真品和赝品的例子，让他自己总结规律。

其实核心就是前文机器学习中我们介绍的那些内容。

这个阶段在一些人脸检测、行人检测以及物体识别等特定的任务上都取得过不错的突破，但还是那个问题，特征的设计需要大量专业知识，导致整体视觉模型开发周期长，面对复杂场景时手工特征就力不从心了。

而第四个阶段就是 **深度学习** 阶段了，这个阶段从 2012 年开始，而这个开始的标志性事件，其实就是 AlexNet 在 ImageNet 挑战赛的夺冠。深度学习阶段的核心就是抛弃人工设计特征，转为了让神经网络自动学习特征，用海量的数据去训练深层网络。

这就引出了我们的主角 —— **卷积神经网络**。

## 4.2.6 卷积神经网络（CNN）

在理解 CNN 之前，我们先了解图像的数字化表示。图像由像素点组成，每个像素点都有对应的数值：灰度图像用 0-255 表示明暗度（0 为黑色，255 为白色），而彩色图像则用红、绿、蓝（RGB）三组 0-255 的数值来表示不同颜色。

在 CNN 出现前，研究者们还尝试用普通的多层全连接神经网络处理图像。想象一下，对于一张 224×224 像素的彩色图片，它的总输入是 224×224×3（RGB）=150528，那么对于普通的多层全连接神经网络，如果第一层有 1000 个神经元，就需要 150528×1000=1.5 亿个连接（如图 4-4），这不仅导致参数量过大容易过拟合，还忽视了图像的空间结构特征，因为相邻像素和远距离像素被同等对待。

![图 4-4 全连接神经网络第一层连接示意图](https://cdn.isboyjc.com/ai-evolution/1756138174865.png)

CNN 的设计灵感来自人类视觉系统。生物学研究发现，视觉皮层的神经元只对视野中的特定小区域敏感：简单细胞负责检测边缘和线条，复杂细胞则识别更高级的形状。这种局部感知机制在整个视野中都是相似的。

卷积神经网络，最重要的自然是“卷积”了，它听起来很抽象，其实很好理解。想象你在寻找草地上的特定花朵：你会用一个小框子，从左上角开始，逐步扫描整片草地的每个局部区域。CNN 中的卷积操作就是用这种方式工作的，其中的 **卷积核（Kernel）** 就相当于这个“小框子”。

为了简单演示 CNN 的工作原理，我们以最基础的黑白二值图像为例（像素只有黑和白两种颜色）。

假设我们要训练一个 CNN 识别字母 X。输入一张 7×7 的黑白图片时，我们先将其转换为数字：白色像素用1表示，黑色像素用 -1 表示（如图 4-5）。

![图 4-5 输入图像X图示](https://cdn.isboyjc.com/ai-evolution/1756138544015.png)

接下来，我们用 3 个 3×3 的卷积核来提取图片的局部特征（如图 4-6）。

![图 4-6 预设卷积核图示](https://cdn.isboyjc.com/ai-evolution/1756138532548.png)

这 3 个卷积核代表了输入图像X的特征。需要说明的是，这只是为了便于理解。在实际应用中，卷积核的初始值是随机的，通过训练过程不断优化，最终形成能有效提取特征的“理想”卷积核。

当图片输入 CNN 时，首先经过 **卷积层（Convolutional Layer，简称Conv Layer）** 处理得到 **特征图（Feature Map）**。卷积核会在图像上从左到右、从上到下滑动，每次停留时都进行一次计算：将卷积核的数值与对应位置的像素值相乘并求和，得到特征图中的一个值。

让我们用 1 号卷积核演示这个过程。首先处理图片左上角的 3×3 区域：将这个区域与卷积核对应位置的值相乘后求和（为了便于理解，这里还进行了平均值处理，标准卷积运算只需点乘求和，如图 3-33）。计算过程如下：

```
((1×1)+(-1×-1)+(-1×-1)+(-1×-1)+(1×1)+(-1×-1)+(-1×-1)+(-1×-1)+(1×1))/9=1
```

这个区域的计算结果是 1，这是最大可能值，表示完全匹配。在特征图中，我们用不同深浅的灰度表示数值大小，1对应纯白色。

![图 4-7 1号卷积核卷积运算第一步](https://cdn.isboyjc.com/ai-evolution/1756138518533.png)

卷积核继续向右滑动一个单元格（步长为 1），同样进行点乘累加求平均运算，得到计算结果约为 -0.11（如图 4-8）。

![图 4-8 1号卷积核卷积运算第二步](https://cdn.isboyjc.com/ai-evolution/1756138506593.png)

按照这种方式，卷积核先完成第一行的计算，然后下移一格，再次从左至右扫描计算，最终完成整个图像区域的运算（如图 4-9）。

![图 4-9 1号卷积核卷积运算结果](https://cdn.isboyjc.com/ai-evolution/1756138494014.png)

观察输出的特征图可以发现，图像边缘的一圈信息未被计算。为了不遗漏这些边缘信息，我们需要进行 **填充（Padding）**处理，即在原图四周添加一圈值为 -1 的黑色像素，这样就能完整计算原图的所有区域（如图 4-10）。

![图 4-10 Padding操作再次计算示意图](https://cdn.isboyjc.com/ai-evolution/1756138481533.png)

通过这个过程，我们获得了基于1号卷积核的特征图（如图 4-11）。这其实就是最简单的 **卷积运算（Convolution Operation）**。

![图 4-11 1号卷积核卷积运算结果](https://cdn.isboyjc.com/ai-evolution/1756138467722.png)

在卷积层进行卷积运算得到特征图之后，我们需要引入非线性变换，这就要用到 **ReLU（修正线性单元）激活函数**。它的原理非常简单：

- 如果特征图中的值大于 0，保持不变
- 如果值小于或等于 0，就将其变为 0

负值被清零，保留了正的特征响应，这就像是在图像处理中保留了显著的特征，而将不明显的特征直接屏蔽掉（如图 4-12）。

![图 4-12 ReLU 激活函数](https://cdn.isboyjc.com/ai-evolution/1756138455008.png)

接下来进入 **池化层（Pooling Layer）**，对特征图进行池化处理，它的作用是压缩数据和提取主要特征。最常用的是 **最大池化（Max Pooling）**，当然还有 **平均池化（Average Pooling）**，这里我们选择最大池化（如图 4-13），它的工作方式如下：

- 用一个 2×2 的小窗口在特征图上滑动（通常步长为 2）
- 在每个 2×2 的区域中，只保留最大的那个数值
- 这样特征图的尺寸就会减小一半，但保留了最重要的特征信息，降低噪声影响

![图 4-13 最大池化](https://cdn.isboyjc.com/ai-evolution/1756138441018.png)

通过这些操作的组合（**卷积-ReLU-池化**）就构成了 CNN 的基本处理单元，通过多次重复这样的处理，网络就能逐层提取越来越抽象的特征。

在完成 **卷积-ReLU-池化** 的操作后，得到的特征图还是二维的数据。为了能够进行最终的分类，我们需要进行 **展平（Flatten）** 操作，将二维的特征图转换为一维向量，如上述流程中得到的4×4的特征图会变成一个包含 16 个数字的一维数组。这就像把一张照片上的所有信息都排成一条线（如图 4-14）。

![图 4-14 二维特征图一维化](https://cdn.isboyjc.com/ai-evolution/1756138427461.png)

注意，我们仅使用 1 号卷积核处理后的特征图做示例，使用相同的计算方法，我们同样可以得到 2 号和 3 号卷积核对应的特征图，最终它们展平后会组合成一个一维数组（3 个卷积核，每个输出 4×4 的特征图，展平后会得到一个包含 48 个数字（3×4×4=48）的一维数组）。

接下来将展平的数据送入 **全连接层（Fully Connected Layer，简称FC Layer）**。这其实就是我们前文介绍过的全连接神经网络，还记得神经网络抽象公式吗？这里的计算方式也类似，想象一下，前面提取的所有特征现在要经过一个“大脑”来做出最终判断。在全连接层中每个神经元都与上一层的所有神经元建立连接，就像一张密密麻麻的网，每个连接都有一个权重值，表示这个特征的重要程度。神经元会将接收到的所有信息加权求和，然后进入输出层（如图 4-15）。

![图 4-15 全连接示意图](https://cdn.isboyjc.com/ai-evolution/1756138414040.png)

输出层通常使用 **Softmax激活函数**，它会将神经网络的输出转换为概率分布。比如，如果是在识别数字 0-9，输出层就会输出 10 个概率值，每个数字对应一个概率，所有概率加起来等于 1。概率最高的那个就是网络认为最可能的答案。

这样，整个 CNN 就像一条完整的流水线：从输入图像开始，通过多层卷积提取特征，然后将这些特征综合分析，最终得出预测结果（如图 4-16）。

![图 4-16 CNN流程图](https://cdn.isboyjc.com/ai-evolution/1756138399259.png)

如果把整个 CNN 流程简化使用结构图表示的话，其实就是 **输入层-卷积层-ReLU-池化层-全连接层-输出层**（如图 4-17）。

![图 4-17 CNN简化结构图](https://cdn.isboyjc.com/ai-evolution/1756138384984.png)

CNN 开始时会随机初始化卷积核的参数。这些卷积核就像一个个“探测器”，最初并不知道要寻找什么样的特征。输入层将图像转换为数字矩阵，比如RGB图像会转为三通道的数值矩阵。卷积层中，卷积核在图像上滑动，通过点乘求和提取局部特征，生成特征图。ReLU 激活函数对特征图进行非线性变换，保留正值特征，将负值置零。池化层使用最大池化等方法压缩数据，保留重要特征。全连接层将特征图展平后输入全连接神经网络，进行特征计算。最后输出层使用 Softmax 函数将结果转换为概率分布。

通过大量标注好的图像数据反复训练，CNN 会使用反向传播算法不断调整卷积核的参数。当网络预测结果与实际标签不符时，误差会从输出层通过反向传播逐层向前传递，计算每一层参数对误差的贡献，并据此更新参数。随着训练进行，卷积核会逐渐“学会”提取对任务有用的特征。比如在识别猫的任务中，浅层卷积核可能学会检测边缘和纹理，深层卷积核则可能学会识别眼睛、耳朵等更复杂的特征。这个过程就像训练一个层层递进的视觉系统，从最基础的特征开始，逐步建立起对复杂图像的理解能力。这种从原始输入直接到最终输出的设计真正实现了 **端到端学习（End-to-end Learning）**，不需要人工设计中间特征。

而 AlexNet 其实就是一个 8 层的 CNN，准确来说其中 5 层是卷积+池化及 3 层全连接神经网络（如图 4-18）。

![图4-18 AlexNet结构图](https://cdn.isboyjc.com/ai-evolution/1756138373778.png)

CNN 的雏形可追溯至 1980 年 **福岛邦彦（Kunihiko Fukushima）** 提出的 **神经认知机（Neocognitron）**，1989-1998 年间 **杨·勒丘恩（Yann LeCun）** 将反向传播算法与卷积结构结合正式提出这一架构，但受限于计算资源和数据规模，CNN 的潜力直到 2012 年 AlexNet 在 ImageNet 竞赛中的突破性表现才真正展示了它的巨大潜力。

除了采用 CNN 作为核心架构，AlexNet 还在硬件创新上取得了突破。克里泽夫斯基团队使用两块 NVIDIA GTX 580 显卡进行并行训练，成为首个在大规模视觉任务中成功应用 GPU 并行计算的模型。这一创新将训练时间从传统 CPU 的数月缩短至5-6天，同时显著提升了计算效率。这也为深度学习模型后续广泛依赖 GPU 计算奠定了基础。