---
title: 第五章 Transformer时代
---

本章将我们带入由 Transformer 架构开启的人工智能新纪元。这一时期，AI 的发展范式发生了根本性的转变，从针对特定任务设计专门模型，转向构建具备通用能力的大规模预训练模型。

故事的开端是 2017 年 **Transformer** 架构的横空出世。它彻底摒弃了 RNN 的串行计算模式，完全依赖于创新的 **自注意力机制**，实现了高效的并行计算，完美解决了长距离依赖问题，为序列建模带来了革命性的突破。

基于 Transformer 的强大能力，**“预训练+微调”** 的范式迅速确立。以 **GPT** 和 **BERT** 为代表的模型，通过在海量无标签文本数据上进行自监督学习，掌握了通用的语言知识，然后在各种下游任务中仅需少量微调即可取得惊人效果，重塑了整个自然语言处理领域。这一范式也被成功应用于代码领域，催生了 **CodeBERT** 等模型，让 AI 开始真正“理解”程序逻辑。

随后，研究者们发现了惊人的 **“规模效应”**：模型越大，性能越好。更重要的是，当模型规模跨越某个临界点后，会自发产生未被直接训练过的全新能力，即 **“涌现”**，如上下文学习、代码生成和数学推理。以 **GPT-3** 为代表的巨型模型的成功，引发了一场全球性的“军备竞赛”。

然而，大模型的强大通用能力和不可预测的涌现现象，也带来了全新的挑战：

**“如何确保这些庞大而复杂的AI系统是可控、可靠且对人类有益的？”** 

这个问题成为了 AI 发展进入新阶段后，整个领域必须面对和解决的核心议题。


