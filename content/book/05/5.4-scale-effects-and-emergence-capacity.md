---
title: 5.4 规模效应与涌现能力
description: 随着模型参数和数据量的指数级增长，研究者发现了“缩放定律”——模型越大，性能越好。更重要的是，当规模达到临界点时，模型会表现出未被直接训练的“涌现能力”，如上下文学习、代码生成和数学推理。以 GPT-3 为代表的模型的成功，引发了全球性的“规模竞赛”，开启了AI领域对更大、更强模型的探索。
---

随着 Transformer 架构和预训练范式的确立，研究者开始探索：如果继续扩大模型规模，增加参数数量、训练数据和计算时间会带来怎样的影响？

## 5.4.1 大就是好

我们可以把参数想象成模型大脑中的“神经连接”。每个参数都是一个可以调整的数值，用来控制信息如何在神经网络中流动和处理。参数越多，模型的“大脑容量”就越大，理论上能够学习和记忆的知识就越丰富。

而训练数据则是用来“教育”模型的资料，就像给学生提供的教科书和参考资料。模型通过阅读海量信息来学习语言规律、知识事实和推理模式。训练数据越多越丰富，模型学到的知识就越广博，一个博览群书的学者肯定比只读过几本书的人更有学问。

2018、2019、2020 年，OpenAI 陆续发布了 GPT 的三个版本，对比来看它们的参数以及训练规模（如表 5-1）。

| **模型** | **发布时间** | **参数量** | **训练数据****规模**                                         |
| -------- | ------------ | ---------- | ------------------------------------------------------------ |
| GPT-1    | 2018年       | 117M       | 40GB                                                         |
| GPT-2    | 2019年       | 1.5B       | 40GB                                                         |
| GPT-3    | 2020年       | 175B       | 570GB (约45TB的原始文本经过预处理后有效数据量大约是这个级别) |

*表 5-1 GPT-1、GPT-2、GPT-3规模对比*

可以看到从 GPT-1 到 GPT-3，参数量增长了约 1500 倍，训练数据量增长了约 1000 倍。

当研究者们系统地研究模型规模与性能的关系时，他们发现了一个惊人的规律：模型性能与参数量、数据量、计算量之间存在幂律关系，在数学上可以称之为 **缩放定律（Scaling Laws）**，数学公式如下：

```math
L(N) = (N_c/N)^{\alpha_N}
```

我们不去介绍复杂的数学公式，简单来说，这个公式告诉我们：**模型越大，效果越好，而且这种关系是可预测的。**

使用语言建模任务和 SuperGLUE 基准测试两组数据来看下他们的性能对比（如表 5-2），在语言建模任务中通常使用 **困惑度（Perplexity）** 来表示其性能，困惑度当然是越低越好。而 SuperGLUE 基准测试是 NLP 领域的重要评测基准，旨在评估大规模预训练语言模型在复杂语言理解任务上的性能。

| **任务**      | **GPT-1** | **GPT-2** | **GPT-3** | **备注**       |
| ------------- | --------- | --------- | --------- | -------------- |
| 困惑度        | ≈35       | ≈20       | ≈15       | 数值越低越好   |
| SuperGLUE分数 | 约30分    | 约50分    | 约71分    | 人类水平约89分 |

*表 5-2 GPT-1、GPT-2、GPT-3性能对比*

从这些数据我们可以清晰地看到，随着模型规模的扩大，性能确实在持续提升。

## 5.4.2 涌现

规模效应最令人震惊的不是性能的线性提升，而是当达到一定规模时产生的质变。

当模型规模达到某个临界点时，会突然出现之前没有的新能力。这种现象被称为 **涌现（Emergence）**，即系统整体表现出的、单个组成部分所不具备的性质或能力。这些能力并不是通过专门训练得来的，而是自发产生的。

涌现能力有几个特点：

- **突发性：** 能力不是逐渐增强，而是某个规模突然出现
- **不可预测：** 无法准确预测具体能力和出现时间
- **质的跃变：** 不是量的改善，而是本质上的新能力

GPT-3 的出现就展示出了令人惊讶的涌现能力。

### （1）上下文学习（In-Context Learning）

GPT-3 可以通过给定少量示例，理解并执行新任务，这意味着它无需专门去训练就能够即插即用。

```
输入:
English: Hello → Chinese: 你好
English: Thank you → Chinese: 谢谢
English: Good morning → Chinese:

输出:
早上好
```

### （2）代码生成

虽然 GPT‑3 主要用自然语言训练，但它展现出了强大的代码生成能力。

```python
# 需求: "写一个Python函数，计算斐波那契数列的第n项"

# GPT-3 回答
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
```

### （3）数学推理

GPT-3 能够进行多步骤的数学推理，分步计算并且一步步的推导出结果，无需显式编程。

```
输入: "小明有24个苹果，他给了小红1/3，又给了小李1/4。请问小明还剩多少个苹果？"

输出: "小明原有24个苹果
      给小红：24 × 1/3 = 8个
      给小李：24 × 1/4 = 6个  
      总共给出：8 + 6 = 14个
      剩余：24 - 14 = 10个苹果"
```

### （4）创造性写作

GPT3 还可以生成诗歌和趣味故事，展现出了一定的创造力。

```
输入: "写一首关于人工智能的诗"

GPT-3输出:
"在数据的海洋里游弋，
在算法的森林中思考，
我是人类智慧的映射，
也是未来世界的探索者。"
```


## 5.4.3 涌现能力的影响

GPT-3 展现出强大的涌现能力后，研究者们开始深入研究涌现的临界规模（门槛）。研究发现，不同的涌现能力出现的门槛也有所不同。比如，基础的上下文学习能力可能在 10 亿参数左右就开始发挥作用，而生成代码的能力则更倾向于出现在300–500亿参数区间；复杂数学推理能力通常需要 1000 亿参数及以上才能稳定展现。这种现象类似于物理学中的相变：液态受冷凝固为固态，能力并非渐渐增强，而是一旦“临界温度”触发，质变便会发生。

GPT-3 的成功特别是涌现能力的展现，引发了一场全球性的“规模竞赛”。

全球的头部公司几乎同时开始加入了这场规模竞赛，参数量、训练数据规模好像成为了头部公司能力的体现。Google 的 Switch Transformer 模型参数高达 1.6 万亿；DeepMind 的 Gopher 模型有 2800 亿参数；Meta 的 OPT 系列也达到了 1750 亿；国内的阿里通义、百度文心、腾讯混元等模型也纷纷进入百亿、千亿阵营。

规模增长当然是有代价的，它伴随着巨大的计算成本，GPT-3 的训练超过了 400 万美元，而且大型语言模型的能耗非常之高，只有少量拥有大量资源的团队才能支撑起超大规模模型的训练，技术门槛很高。

当然，面对庞大规模带来的高昂成本与环境压力，部分研究者们也开展了很多效率和可持续性的优化尝试，从而产生了 **知识蒸馏**、 **参数剪枝**、 **低精度量化**、 **混合精度训练**、 **梯度累积**、 **模型并行**，以及 **稀疏激活机制** 和 **混合专家架构** 等等技术。这些技术帮助我们在削减资源消耗的同时，让大模型继续保持性能优势。

大模型的涌现能力虽然带来了很多惊喜，但同样也有很多问题存在。因为我们并不清楚涌现能力的具体机制，也难预测下一个临界能力会是什么，更无法精准控制这些能力何时出现。如果认知能力可以涌现，意识是否也可能突然出现？如何确保涌现的能力始终对人类有益？