---
title: 3.5 机器学习走向台前
description: 随着数据量的激增和计算能力的提升，机器学习走向舞台中央。决策树算法（如ID3和更先进的C4.5）因其直观易懂而普及。随后，以随机森林为代表的集成学习方法通过“集体智慧”显著提升了预测的准确性和稳定性。与此同时，K-means等聚类算法在无监督学习领域大放异彩，能够自动发现数据中的隐藏模式。这些算法的成功催生了推荐系统等大规模商业应用，标志着机器学习进入产业化浪潮。
---

1990~2006年是机器学习的黄金时期。

随着统计学习理论的日趋成熟和计算机性能的大幅提升，基于数据驱动的机器学习算法在实际应用中展现出了前所未有的潜力。

这一时期不仅见证了机器学习技术的蓬勃发展，更标志着人工智能领域从实验室研究正式迈入产业化应用阶段。

## 3.5.1 决策树

决策树（decision tree）是一种监督学习算法，它是机器学习中最直观、最易于理解的算法之一，因为它的整个决策过程很像人类，都通过一系列“是/否”问题判断来做出最终决断。

决策树对人工智能的影响非常大，那么接下来，请你拿出纸笔，让我们一起学习决策树相关算法。

### 1. ID3 算法

早在1986年，澳大利亚计算机科学家罗斯·昆兰（Ross Quinlan）就提出了一种决策树算法——ID3（iterative dichotomiser 3），它是一种贪婪算法，自顶向下构建决策树，核心思想是选择能最大化信息增益的特征作为决策点（节点）。

#### （1）什么是？

我们可以用熵（entropy）来衡量一组数据的混乱程度或不确定性。熵值越低，数据越“纯”；熵值越高，数据越“混”。熵值的计算公式如下：

```math
Entropy(S) = -\sum_{i=1}^{n} x_i \log_2 x_i 
```

其中，$$x_i$$ 是数据集中第i类样本所占的比例（概率）。

我们通过一个例子来理解这个熵，假设有10个球，10个球都是黑球，那么这组数据的熵值为0，因为数据很“纯”；如果10个球中有5个黑球、5个红球，那么这组数据的熵值为1，因为只有两种球且均匀分布；如果熵值大于1，那么数据集类别一定超过2种。

而信息增益（information gain）则用来衡量按照某个特征分割数据后，数据集整体熵值的减少量，减少得越多说明信息增益越大，从而说明这个特征越有效，因为这个特征可以帮我们更快、更好地把数据分离得更“纯”。

#### （2）过程示例

我们以一个“银行贷款审批”的例子结合来理解这一算法。

“想象你是银行的贷款审批专员，你的工作是根据客户的信息决定是否批准贷款，表3-6提供了一组往期银行贷款客户申请数据。”

| **姓名** | **年收入****/元** | **房产价****值/元** | **信用记录****/分** | **审批结果** |
| :------- | :---------------- | :------------------ | :------------------ | :----------- |
| 张三     | 14万              | 60万                | 750                 | 批准         |
| 李四     | 8万               | 30万                | 680                 | 拒绝         |
| 王五     | 16万              | 40万                | 680                 | 拒绝         |
| 赵六     | 15万              | 80万                | 680                 | 批准         |
| 刘七     | 7万               | 70万                | 650                 | 拒绝         |
| 陈八     | 18万              | 35万                | 750                 | 批准         |

*表3-6 往期银行贷款客户申请数据*

在表3-6所示的6个样本数据中，年收入、房产价值和信用记录都属于数据特征，如果使用ID3算法，将按照以下步骤构建决策树。

首先ID3只能处理离散特征，所以需要人工将所有特征离散化，例如把年收入的数值替换成“高收入”“低收入”：

- 年收入<12万元为“低收入”，≥12万元则为“高收入”
- 房产价值<50万元为“低价值”，≥50万元则为“高价值”
- 信用记录<700分为“差信用”，≥700分则为"好信用”

根据这个标准，表3-6中的有效样本分类如下：

- 年收入：李四、刘七为低收入，张三、王五、赵六、陈八为高收入。
- 房产价值：李四、王五、陈八为低价值，张三、赵六、刘七为高价值。
- 信用记录：李四、王五、赵六、刘七为差信用，张三、陈八为好信用。

第二步是找到一个最佳的特征作为切入点（决策树根节点），那么如何找这个最佳特征，就需要计算信息增益。

我们简单做个示例看下最佳特征是如何计算的。

在整个（6个）样本数据中审批结果中3批准3拒绝，所以样本数据基础熵值是1。

**按照年收入计算（6个有效样本）：**

- 低收入组（李四、刘七）：2人中0个批准2个拒绝，熵值为0，计算过程如下
  - $$-\sum_{i=1}^{n} x_i \log_2 x_i = -0 \cdot \log_2 0 - 1 \cdot \log_2 1 = 0$$
- 高收入组（张三、王五、赵六、陈八）：4人中3个批准1个拒绝，熵值为0.811，计算过程如下
  - $$-\sum_{i=1}^{n} x_i \log_2 x_i = -(3/4) \cdot \log_2 (3/4) - (1/4) \cdot \log_2 (1/4) = 0.811$$
- 共2组6个样本，它们的加权熵为0.541，计算过程如下
  - $$2/6 \cdot 0 + 4/6 \cdot 0.811 = 0.541$$
- 信息增益 = 初始熵(1) - 加权熵(0.541) = 0.459

**按照房产价值计算（6个有效样本）：**

- 低价值组（李四、王五、陈八）：3人中1个批准2个拒绝，熵值为0.918，计算过程如下
  - $$-\sum_{i=1}^{n} x_i \log_2 x_i = -(1/3) \cdot \log_2 (1/3) - (2/3) \cdot \log_2 (2/3) = 0.918$$
- 高价值组（张三、赵六、刘七）：3人中2个批准1个拒绝，熵值为0.918，计算过程如下
  - $$-\sum_{i=1}^{n} x_i \log_2 x_i = -(2/3) \cdot \log_2 (2/3) - (1/3) \cdot \log_2 (1/3) = 0.918$$
- 共2组6个样本，它们的加权熵为0.918，计算过程如下
  - $$3/6 \cdot 0.918 + 3/6 \cdot 0.918 = 0.918$$
- 信息增益 = 初始熵(1) - 加权熵(0.918) = 0.082

**按照信用记录计算（6个有效样本）：**

- 差信用组（李四、王五、赵六、刘七）：4人中1个批准3个拒绝，熵值为0.811，计算过程如下
  - $$-\sum_{i=1}^{n} x_i \log_2 x_i = -(1/4) \cdot \log_2 (1/4) - (3/4) \cdot \log_2 (3/4) = 0.811$$
- 好信用组（张三、陈八）：2人中2个批准0个拒绝，熵值为0，计算过程如下
  - $$-\sum_{i=1}^{n} x_i \log_2 x_i = -1 \cdot \log_2 1 - 0 \cdot \log_2 0 = 0$$
- 共2组6个样本，它们的加权熵为0.541，计算过程如下
  - $$4/6 \cdot 0.811 + 2/6 \cdot 0 = 0.541$$
- 信息增益 = 初始熵(1) - 加权熵(0.541) = 0.459

从计算结果来看，按照年收入和信用记录计算得到的信息增益都是0.459，均高于根据房产价值计算得到的信息增益0.082。在这种情况下，我们可以任选其一作为根节点，这里我们选择年收入作为根节点。那么决策树第一层就是判断收入水平：

- 高收入：（张三、王五、赵六、陈八）：4人中3个批准1个拒绝
- 低收入：（李四、刘七）：2人中0个批准2个拒绝

由于低收入组2人全部是“拒绝”，所以无需继续分割数据，但是高收入组4人有3个批准1个拒绝，所以需要以高收入组的数据为样本继续分割。

我们接着计算信息增益来确定第二层节点的特征选择，对高收入组的4个样本（张三、王五、赵六、陈八）进行计算：

首先计算这4个样本的基础熵值：4人中3个批准1个拒绝，基础熵值为0.811

**按照信用记录计算：**

- 好信用组（张三、陈八）：2人中2个批准0个拒绝，熵值为0
- 差信用组（王五、赵六）：2人中1个批准1个拒绝，熵值为1
- 加权熵=2/4×0+2/4×1=0.5
- 信息增益=0.811-0.5=0.311

**按照房产价值计算：**

- 低价值组（王五、陈八）：2人中1个批准1个拒绝，熵值为1
- 高价值组（张三、赵六）：2人中2个批准0个拒绝，熵值为0
- 加权熵 = 2/4 × 1 + 2/4 × 0 = 0.5
- 信息增益 = 0.811 - 0.5 = 0.311

从计算结果可以看出，按照信用记录和房产价值计算得到的信息增益相同（0.311），因此，我们随机选择信用记录作为第二层节点：

- 好信用组（张三、陈八）：2人中2个批准0个拒绝
- 差信用组（王五、赵六）：2人中1个批准1个拒绝

由于好信用组2人全部批准审批，所以无需继续分割，但是差信用组中2人中1个批准1个拒绝，所以还需要继续分割，所以最后一层就是按照房产价值来分割。

基于表3-6所示的样本数据，按照上述流程，ID3算法生成了一个三层决策树（如图3-12）。

![图3-12 ID3算法进行银行贷款审批的决策树结构](https://cdn.isboyjc.com/ai-evolution/1756137617179.png)

从图3-12可以看出，银行的贷款审批专员以年收入为根节点来构建决策树，直接拒绝低收入客户，对高收入客户需要进一步根据信用记录和房产价值来判断。对于高收入客户，如果是好信用（≥700分），则批准贷款；如果是差信用（<700分），则需要进一步判断房产价值，房产价值为高价值（≥50万）时批准贷款，为低价值（<50万）时拒绝贷款。这个判断过程形成了一个三层的决策树结构。

这就是ID3算法的完整工作流程，层层分割，直到每个叶子节点都是纯净的（或达到停止条件）。

#### （3）缺陷

当然，大家可能有注意到，ID3算法有很多问题。

**连续型数据处理具有主观性**

ID3算法只能处理离散型数据，针对连续型数据（数值），依赖人工给特征数据分类（如年收入<12万元为"低收入"，反之则是"高收入"），一旦由人工分类，这个分类的界限就全由人类主观判断了。

**缺失值处理方式不合理**

表3-6中的样本数据都是完整的，假如张三的房产价值数据是空白项，那么对ID3算法来说，它的解决方式有以下3种，但哪一种都不够科学。

- 删除张三的整组样本数据。
- 用所有样本数据中房产价值的平均值填充。
- 人工判断。

**无意义特征干扰**

ID3算法最大的问题就是比较容易被欺骗，假如我们给表3-6中的样本数据新增一列身份证号数据，那么每个身份证号都会被当作身份证这个特征的一类，有几个身份证号，就划分了几个类别，而每个身份证号类别对应的要么是“批准”、要么是“拒绝”。所以按照身份证号计算得到它的信息增益一定是最大的，这会导致整个算法失去意义。对照3.4.1节提到的拟合、泛化概念，身份证号这种特征能完美拟合任何训练数据，但是完全没有泛化能力（对于训练数据，分类准确为100%，但对于新的数据，分类准确率为0%）。

### 2. C4.5 算法

面对ID3算法的种种问题，1993年，罗斯·昆兰又提出了C4.5算法。C4.5是ID3的进阶版本，它巧妙地解决了ID3的三大缺陷。

我们还是使用银行贷款审批的例子，看看C4.5是如何处理表3-6所示的样本数据的。

由于ID3算法只能处理离散特征，所以需要人工将所有特征离散化，比如把年收入的数值替换成"高收入"、"低收入"。而使用C4.5算法则不需要人为干预，因为它可以自动找到最佳的分割点。C4.5算法自动分割的核心思想很简单：既然年收入是连续数值，那就尝试不同的分割点，看看哪个分割点能够带来最大的信息增益。

#### （1）连续型数据处理

以年收入特征为例，首先，算法会将年收入按照从小到大的顺序排序（7万元、8万元、14万元、15万元、16万元、18万元），然后在相邻的数值中点设置候选分割点（7.5万元、11万元、14.5万元、15.5万元、17万元）。

接着算法会计算每个候选分割点的信息增益。

候选分割点1（年收入>7.5万元）的信息增益计算过程如下：

- 大于7.5万：张三(批准)、王五(拒绝)、赵六(批准)、陈八(批准)，4人3批准1拒绝
- 小于7.5万：李四(拒绝)、刘七(拒绝) ，2人0批准2拒绝
- 信息增益=1-(4/6)×0.811-(2/6)×0=0.459

候选分割点2（年收入>11万）的信息增益计算过程如下：

- 大于11万：张三(批准)、王五(拒绝)、赵六(批准)、陈八(批准) ，4人3批准1拒绝
- 小于11万：李四(拒绝)、刘七(拒绝)，2人0批准2拒绝
- 信息增益=1-(4/6)×0.811-(2/6)×0=0.459

候选分割点3（年收入>14.5万）的信息增益计算过程如下：

- 大于14.5万：赵六(批准)、王五(拒绝)、陈八(批准)，3人2批准1拒绝
- 小于14.5万：张三(批准)、李四(拒绝)、刘七(拒绝)，3人1批准2拒绝
- 信息增益=1-(3/6)×0.918-(3/6)×0.918=0.082

候选分割点4（年收入>15.5万）的信息增益计算过程如下：
- 大于15.5万：王五(拒绝)、陈八(批准)，2人1批准1拒绝
- 小于15.5万：张三(批准)、李四(拒绝)、赵六(批准)、刘七(拒绝) ，4人2批准2拒绝
- 信息增益=1-(2/6)×1-(4/6)×1=0

候选分割点5（年收入>17万）的信息增益计算过程如下：

- 大于17万：陈八(批准)，1人1批准0拒绝
- 小于17万：张三(批准)、李四(拒绝)、王五(拒绝)、赵六(批准)、刘七(拒绝)，5人2批准3拒绝
- 信息增益=1-(1/6)×0-(5/6)×0.971=0.191


最终计算完每个候选分割点的信息增益后，比较所有的候选分割点信息增益：

- 分割点1（7.5万）：0.459
- 分割点2（11万）：0.459
- 分割点3（14.5万）：0.082
- 分割点4（15.5万）：0
- 分割点5（17万）：0.191

由于分割点1和分割点2的信息增益都是最高的（0.459），这时C4.5算法会选择数值比较小的分割点（7.5万元），这样可以保持决策树规则的简洁。那么最终选择年收入7.5万元为分割条件。

如此算法会计算每个特征的分割点无需人工干预，ID3算法的第一个问题就这样被解决了。

#### （2）缺失值处理

使用ID3算法时，假如某个特征有数据空白项，ID3的做法会非常不人性化。C4.5算法采用了一种叫作“分数分割”（split point）的方法来处理缺失值。

假设张三的房产价值数据是空白的，C4.5算法在预训练阶段，会先统计样本的分布情况。

- 总样本数为6个
- 房产价值非缺失样本共5个 
  - 低价值：李四、王五、陈八（3人）
  - 高价值：赵六、刘七（2人）
- 分布比例：低价值60%，高价值40%

当算法发现张三的房产价值数据是空白的时，会按照事先统计的分布比例给缺失的张三的房产价值数据分配权重，张三原本权重是1，重新分配后权重如下。

- 分配给低价值：张三权重0.6
- 分配给高价值：张三权重0.4

注意，这并不是真的把张三分成了两个人，而是在计算时把张三的“影响力”按比例分摊了，当计算信息增益时，原来的6个样本数据，现在就变成了如下分布。

- 低价值：李四(1.0)、王五(1.0)、陈八(1.0)、张三(0.6)=3.6个样本数据
- 高价值：赵六(1.0)、刘七(1.0)、张三(0.4)=2.4个样本数据

显然这样分摊之后，信息增益的计算会更合理，因为这并不是随意猜测的，而是基于已知数据的实际分布比例分摊，也没有丢弃任何样本数据，最大化地利用了可用样本数据，预测阶段的处理方式和训练阶段是完全一致的。


#### （3）无意义特征处理

ID3算法中最大的问题是无意义的特征干扰，如上文中的身份证号特征问题。C4.5算法面对这种欺骗性特征，引入了一个“增益率”（gain ratio）的概念：


```
增益率 = 信息增益 / 分割信息量
```

用“分割信息量”来衡量特征分割的复杂程度。

对于身份证号特征问题，虽然按照身份证号计算得到的信息增益非常高，但是由于每个身份证号都属于一类，所以有多少身份证号就有多少类，分割信息量也很高，代入增益率公式得出的增益率仍然是低的。这样一来，C4.5算法就完美避免了无意义特征的干扰。


#### （4）剪枝（Pruning）

到这里，C4.5算法就已经解决了ID3算法的核心缺陷。当然，它不止于此，C4.5算法的另一个重要改进是引入了“剪枝”（pruning）技术来防止过拟合。

前文中简单介绍过过拟合的概念。想象一种极端情况，最终的决策树非常复杂，样本数据中有非常多特征，最终的决策树为每个特征都建立了分支，那么整个决策树的层级将会非常深。例如，在银行贷款审批的样本数据中，哪怕是为所有特征都建立分支，整个决策树也只有3层，但是假如决策树有10层、50层，甚至100层呢？这种训练出来层级比较深的决策树，可能在训练数据上的表现可以达到100%的准确率，但是对新样本数据的预测能力会很差，因为它过渡拟合了训练数据的细节。

C4.5算法使用了剪枝技术中的“后剪枝”（Post-pruning）方案来改善过拟合问题。先按照算法流程构建一个完整的决策树，决策树构建完成之后，从树的最底层开始，逐级评估是否可以删除某个分支，它会尝试删除掉某个分支，然后用验证数据集（一批不在训练样本数据集中的数据）来测试剪枝后的决策树的错误率。

在银行贷款审批示例中，假设我们有以下分支：

- 高收入，判断信用记录>700？ 
  - 是：批准（训练数据的分类准确率为90%，验证数据的分类准确率为75%）。
  - 否：拒绝（训练数据的分类准确率为95%，验证数据的分类准确率为70%）。

如果删除信用记录这个分支，直接用它的上层“高收入直接批准”来验证，最终训练数据的分类准确率为85%，验证数据的分类准确率为82%。虽然训练数据的分类准确率下降了，但是验证数据的分类准确率却提高了，说明这个剪枝是有效的。因为对我们来说，基于样本数据训练这样一个决策树就是为了帮我们提升工作效率或者说免去人力，最终是要拿新数据给决策树判断的，所以验证准确率提升对我们来说是好的。

对复杂决策树进行剪枝之后，减少了对训练数据的过渡依赖，也就是泛化能力提升了，整体的决策树变得更小，决策时检查的条件也会变少。

C4.5算法的成功标志着机器学习算法开始具备真正的自动化和智能化特征。它不仅解决了实际问题，更重要的是证明了机器可以从数据中自动学习，而且往往比人工规则更加准确和高效。


## 3.5.2 随机森林

虽然C4.5算法已经很强大，能够自动处理连续数据、缺失值，还能剪枝防止过拟合，但单个决策树仍然有一个根本性的局限——容易"钻牛角尖"。

想象一下，你要做一个重要决定，比如是否投资某只股票，你会怎么做？

- Plan A：找一个最聪明的专家，让他给你答案
- Plan B：找100个不同领域的专家，根据他们的结果综合决定

很明显，从直觉上Plan B更靠谱些，这就是随机森林算法的核心思想。

2001年，统计学家利奥·布雷曼（Leo Breiman）提出了随机森林（Random Forest）算法，直接现实化了“三个臭皮匠顶个诸葛亮”的智慧。它是由华裔美国人何天琴于1995年提出的随机决策森林（random decision forests）演变而来。核心思想是通过构建多个决策树并结合多个决策树的预测结果来确定最终的预测结果。

随机森林是一种基于集成学习的监督式机器学习算法，它比较核心的是两个“随机”的概念。


### 1. 特征随机（Feature采样）

想象一下，真实的银行贷款中会有很多特征（年收入、房产价值、信用记录、年龄、工作年限、婚姻状况等等）。

传统决策树会考虑所有特征，最终生成一个庞大的决策树。而随机森林会训练多个决策树，每个决策树值关注部分特征，这样每个决策树都只侧重几个特征，最终的决策树就像是“专家”一样，都有自己的“专业领域”，这避免了某些强势的特征主导都有的决策。


### 2. 样本随机（Bootstrap采样）

假设银行有1000个历史贷款申请案例样本数据，现在要训练50个决策树（50个“专家”），你会怎么做，怎么做最准确呢？

传统做法是让每个专家都学习这1000个案例，而随机森林的做法是每个专家都随机抽取800个数据，允许重复数据（随机抽取过程中同一条数据可能被重复抽取到），这样每个专家看到的训练数据都不完全一样，避免了所有的决策树犯某种同样的问题。


### 3. 过程示例

让我们回到银行贷款审批的例子做个演示，使用C4.5算法时，我们基于一组数据集训练出了一个多层级的决策树来确定是否通过审批。我们来看看如果用随机森林是怎么工作的。

随机森林算法需要数据集中有更多的特征，但它并不会根据所有特征生成一个庞大的决策树，而是采用一组一组的特征去训练出多个决策树。

假设我们有1000条样本数据，每个数据有10种特征，拿出10个简单特征组合去训练出10个决策树，每个决策树的侧重点不一样，最终的每个决策树都是不同的“专家”。如下：

- 专家1：年收入+信用记录双特征组合，随机抽取800条作为样本数据
- 专家2：房产价值+年龄双特征组合，随机抽取800条作为样本数据
- 专家3：年收入+工作年限双特征组合，随机抽取800条作为样本数据
- …
- 专家10：信用记录+房产价值双特证组合，随机抽取800条作为样本数据

根据决策树算法，我们训练出了10个专家，随机森林算法会采用多个专家投票决策的机制来确定最终的结果，所以当一个新的客户贷款申请进入随机森林决策流程后，会先经过10个决策树得到10个意见，假如有6个决策树批准贷款，4个决策树拒绝贷款，那么最终会以6:4批准客户的贷款请求。

在上述的例子中，每个决策树可能都不是那么“完美”，但是10个不完美决策树组合的综合投票结果，会比一个“完美”的决策树更好。

其实在现实生活中非常常见，比如我们去看医生，多科室专家会诊一定是比单个专家医生收费更贵诊断结果也更准确，因为一个专家即使是专家，在自己不擅长的领域就是盲区或者擅长领域的也会有一些偏见。而多科室专家会诊，多个不同领域的专家一起讨论，互相补足、印证的诊断结果往往会更好。

在随机森林中，每个树可能在不同的地方犯错，但很少会出现所有的树在同一地方犯错的可能性，形成了错误互补。单个树的结果可能很“敏感”，但多个树的平均结果更稳定，即减少方差。每个树都是专家，但专业领域不同，组合起来的适应性会更强，也就是提高了泛化能力。

应用领域就大规模应用更是佐证了这一理论算法，比如2003年左右，华尔街开始使用随机森林进行股票分析以及量化预测，效果立竿见影，甚至要比传统的专家分析师团队准确率更高。

随机森林的成功印证了在机器学习中，集体智慧要比个体智慧更可靠。这个理念也为后续的各种集成学习算法的诞生指明了方向，同样它也是现代机器学习中的一个重要理念支柱。

## 3.5.3 聚类算法

决策树、随机森林这样的监督学习算法虽然很强大，但是它们都有一个共同的前提条件：必须有“标准答案”。

### 1. 无监督学习

拿我们之前介绍的银行贷款示例来说，我们的样本数据集中的每一条客户样本，都有批准或拒绝贷款的标准答案，算法根据样本数据的特征以及标准答案去学习最终得到一个标准模型可以去预测新的数据。

但是如果没有标准答案呢？

显然，一旦银行贷款的样本数据中没有是否批准贷款这一列标准答案，决策树、随机森林都没有了用武之地。

这个时候，我们就需要一种新的学习方式——无监督学习，不需要标准答案，无监督学习算法可以根据数据特征自己“摸索”出通用规律。而聚类算法就属于典型的无监督学习算法。

### 2. 分类、聚类、回归

其实在机器学习中一共有三种比较核心的任务，分别是**分类、聚类和回归**。

- **分类（Classification）** 属于监督学习，它依赖预先标记好的训练数据，通过训练模型来预测新数据的类别。
- **聚类（Clustering）** 则是无监督学习，它不需要预先定义好类别标签，而是根据数据本身的相似性自动分组。
- **回归（Regression）** 也是监督学习的一种，但与分类不同的是，回归预测的是连续的数值而不是离散的类别，比如预测房价、温度等等。这三种任务中，分类和聚类都是对数据进行分组，一个是按照已知规则分组，一个是自动发现规则分组；而回归则是去预测具体的数值。

**聚类** 是按照某个特定标准（比如距离远近）把一个数据集分割成不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。通俗来讲聚类后同一类的数据尽可能聚集到一起，不同类数据尽可能的分离。

### 3. K-means 聚类算法

K-means算法就是是一种经典的无监督学习聚类算法，这种算法会将数据集划分为K个簇（Cluster），核心思想是通过迭代优化簇内数据点的相似性。

还是回到我们熟悉的银行场景，如果我们有大量的客户数据，但是我们并不知道应该如何给客户分类，这时就可以用到K-means聚类算法帮我们“发现”客户群体。

假设我们有以下客户数据（如表3-7，涉及数据的内容并不完全由真实算法计算得来，理解原理即可）：

| **客户姓名** | **年收入** | **信用记录** |
| ------------ | ---------- | ------------ |
| 张三         | 15万       | 720分        |
| 李四         | 8万        | 650分        |
| 王五         | 20万       | 680分        |
| 赵六         | 3万        | 480分        |
| 陈八         | 12万       | 660分        |
| 周九         | 22万       | 780分        |
| 吴十         | 4万        | 490分        |
| 郑十一       | 10万       | 670分        |
| 钱十二       | 25万       | 760分        |
| 孙十三       | 5万        | 510分        |
| ...          | ...        | ...          |

*表3-7 往期银行客户数据*

首先初始化我们要设置几个随机组长（聚类中心点），初始化的聚类中心有两种方式，第一种是完全随机，从数据集中得到一个取值范围，在范围内随机生成中心点。第二种是从数据集中随机选择样本数据作为中心点。这里假设我们使用随机生成的方式，随机设置了3个组长，即K=3：

- 中心1（年收入10万，信用记录600分）
- 中心2（年收入25万，信用记录750分）
- 中心3（年收入5w，信用记录500分）

确定组长之后，进入分配阶段，每个客户数据点计算自己和所有组长的距离，距离哪个组长最近，就被划分到那个群体：

- 张三（年收入15万，信用记录720分）→ 距离中心2最近 → 分到群体2
- 李四（年收入8万，信用记录650分）→ 距离中心1最近 → 分到群体1
- 王五（年收入20万，信用记录680分）→ 距离中心2最近 → 分到群体2
- 赵六（年收入3万，信用记录480分）→ 距离中心3最近 → 分到群体3
- …

首次分配完成后，我们得到了3组群体数据，接下来开始重新为每个群体重新选择组长，新的组长是群体内所有数据点的“平均位置”：

- 群体1新中心：所有群体1客户数据的平均值（年收入10万，信用记录660分）
- 群体2新中心：所有群体2客户数据的平均值（年收入22.3万，信用记录740分）
- 群体3新中心：所有群体3客户数据的平均值（年收入4万，信用记录493分）

3个新组长选中之后，我们得到了3个新的中心点，接下来以3个新中心点为基础重新计算分配客户数据点，比如：

- 张三（年收入15万，信用记录720分） 
  - 到中心1的距离：较远
  - 到中心2的距离：最近 ← 仍然分配到群体2
  - 到中心3的距离：很远
- 陈八（年收入12万，信用记录660分） 
  - 到中心1的距离：较近
  - 到中心2的距离：较远
  - 到中心3的距离：很远
  - 从群体2调整到群体1（发生了变化！）
- …

由于重新分配数据后，有数据发生了变动（从一个群体被重新分配到了另一个群体），所以需要为3个群体再次重新选择新组长（计算新的中心点），新的组长诞生后再次重新计算分配数据，数据分配完之后如果有数据变动再次选择组长，就这样反复迭代，直到组长选定之后，重新计算分配数据时，所有的数据依然还在原群体，不需要分配到其他群体，这时我们就认为K-means发现了3个稳定的群体。

针对整个银行客户数据集，最终我们得到了3个稳定的群体（如表3-8）。

| **客户群体**     | **特征**                          |
| ---------------- | --------------------------------- |
| 中等收入稳健客户 | 年收入8-15万，信用记录650-700分   |
| 高价值优质客户   | 年收入20万以上，信用记录750分以上 |
| 高风险潜在客户   | 年收入5万以下，信用记录500分以下  |

*表3-8 银行客户聚类后数据*

K-means聚类算法的最大价值在于它能够自动发现一些我们原本不知道或者说很难界定的点，就比如上面的分析银行客户数据的示例中我们最终得到了3个群体，算法自动发现发现了着3个群体在特征空间中的自然边界，根据这3个群体中的客户数据我们就可以得出结论（如表3-9）。

| **客户群体**     | **特征**                          | **发现**                       | **策略**                    |
| ---------------- | --------------------------------- | ------------------------------ | --------------------------- |
| 中等收入稳健客户 | 年收入8-15万，信用记录650-700分   | 这类客户数量最多，风险适中     | 标准贷款产品，利率中等      |
| 高价值优质客户   | 年收入20万以上，信用记录750分以上 | 这类客户违约率极低，但数量较少 | VIP服务，优惠利率，额度更高 |
| 高风险潜在客户   | 年收入5万以下，信用记录500分以下  | 这类客户风险较高，需要特殊审查 | 严格审查，高利率或拒绝      |

*表3-9 银行客户聚类后数据分析*

对于初始化时K值的选择，当数据量比较小时，K值如果过大会导致每个群体的样本过少，整体的群体划分会有噪声或者过拟合。而当数据量较大时，K值如果过小则会导致群体划分过于粗糙，无法发现更细致的数据特征和模式，整体的群体划分就会欠拟合。

1999年，亚马逊（Amazon）开始大规模使用K-means聚类算法进行客户分层，针对不同的群体推荐不同的商品、根据群体偏好动态调整库存、对不同群体使用不同的定价策略，成果也非常显著，客户满意度以及销售额都增长了很多。

### 4. 监督学习 VS 无监督学习

相比决策树、随机森林这样的监督学习算法需要大量已标记的训练数据，K-means这样的无监督学习算法最大的特点就是"让数据自己说话"，算法自动发现数据中隐藏的结构和模式：

算法能够根据数据的自然分布，找到最合适的分组方式，而不是依赖人为定义的规则。通过将相似的数据点聚集在一起，能够发现一些人工分析难以察觉的数据关联。随着数据的变化，聚类中心会自动调整，适中保持对数据特征的准确捕捉，适应性极强。

这种"让数据说话"的能力在现代机器学习中非常重要，尤其是在处理海量、复杂的数据时。不过需要注意的是，K-means的这种自主发现能力也依赖于一些人为设定的参数（如K值的选择），这也侧面说明了在机器学习中，人类经验和算法能力需要相互结合。


## 3.5.4 产业化浪潮

进入21世纪后，互联网的爆发式增长带来了海量数据，2000年2月全球网站数量约1000万个，同年9月就达到了2000万个，到2006年已经超过1亿，用户行为数据、交易数据、内容数据呈指数级增长，为机器学习算法提供了充足的“食料”。计算能力也大幅提升，摩尔定律依然有效，CPU性能稳步增长，存储成本快速下降，让大规模数据处理成为可能。更重要的是商业价值的显现，早期的成功案例证明了机器学习的实际效果，投资回报率清晰可见，吸引了大量资本和人才投入。

> 摩尔定律：当价格不变时，集成电路上可容纳的晶体管数量约每隔18-24个月增加一倍，性能也随之提升一倍。换言之，单位成本下的计算能力呈指数级增长

机器学习迎来了前所未有的产业化浪潮，各种机器学习算法在多个应用领域落地，在这波浪潮中一个比较重要的变化是各种算法不再单打独斗，而是开始“相互合作”，共同服务于实际产业应用场景。

比如2006年Netflix的推荐系统就面临着一个很大的挑战，如何从大规模的电影数据中精准为每个用户找到最感兴趣的内容？

他们的解决方案不仅仅是选择一个“最好”的算法，而是让多个算法“组团作战”。

第一队是协同过滤算法，该算法主要负责寻找具有相似兴趣的用户群体。基本原理是基于用户的观影历史来预测偏好，比如当发现用户喜欢《泰坦尼克号》和《阿甘正传》时，系统会推荐《肖申克的救赎》这类影片。这种方法的独特优势在于能够为用户发掘出一些意想不到但很可能感兴趣的优质内容。

第二队是内容过滤算法，这种算法着重分析影片本身的属性特征，通过识别用户偏好的演员、类型等具体要素来进行推荐。例如，如果用户经常观看汤姆·克鲁斯主演的动作片，系统就会优先推荐类似特征的电影。这种推荐方式的最大优势是逻辑清晰，容易获得用户的理解和接受。

第三队是矩阵分解算法，这是一种更为复杂的数学建模方法，主要用于挖掘用户和电影之间潜在的关联模式。通过对用户观影行为进行深度分析，它能够有效处理数据稀疏的问题，找出用户的深层次观影规律。

**融合策略——让算法“评分”**

Netflix最终实现的推荐系统不是依赖单个算法，而是综合所有算法的“意见”评分决策出最终结果，例如对《复仇者联盟》的推荐：

- 协同过滤评分：9分
- 内容过滤评分：7分
- 矩阵分解评分：8分
- 最终综合评分：8分（平均分）

这种算法融合的思路迅速在各个应用领域普及开来，从金融风控到电商推荐，从搜索引擎到广告投放，都开始采用多算法协同的方式。原因很简单：现实世界的问题往往复杂多面，单一算法就像只有一种工具的工匠，而算法融合则像是拥有完整工具箱的专家，能够从不同角度分析同一个问题，得出更准确、更可靠的结果。这不仅仅是技术层面的进步，更代表了一种全新的系统思维方式的兴起。